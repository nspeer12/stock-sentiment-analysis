{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Stock Market Tweets\n",
    "\n",
    "- Collect a dataset\n",
    "- Train a model \n",
    "- Predict accuracy of unlabelled tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is straight forward however ***Where do we get data?***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/datasets?search=stock+market+tweet\"><img src=\"kaggle-logo.png\" width=\"150\" style=\"float:left; margin:10px;\"></a><a href=\"https://twitter.com/search?q=%24SQ&src=cashtag_click\"><img src=\"twitter-logo.png\" width=\"150\" style=\"float:left; margin:10px;\"></a><a href=\"https://stocktwits.com/symbol/AAPL\"><img src=\"stocktwits-logo.png\" width=\"250\" style=\"float:left; margin:10px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### The best choice seems to be ***Scraping STOCKTWITS!*** \n",
    "    - Ensure the data is labelled \n",
    "    - Approaches to scraping:\n",
    "    - APIs, Selenium, JSOUP\n",
    "    - GOAL: ***get 1,000,000 labeled tweets***  \n",
    "    - <a href=\"https://api.stocktwits.com/developers/docs\">Stocktwits API limits</a>\n",
    "<br />\n",
    "<br />\n",
    "- ### ***HOW TO COMBAT LIMIT ISSUES?***\n",
    "    - Proxy services such as <a href=\"https://zenscrape.com/#pricingSection\">Proxy Services</a>\n",
    "    - Private computer VPN - Private Internet Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multithreaded StockTwitsScraper\n",
    "# Team: Stock Market Sentiment Analysis\n",
    "# Danish Siddiqui & Stepthen Speer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets on page: 30\n",
      "0 $UPS The Logistics of Disaster Response https://www.otcdynamics.com/ups-the-logistics-of-disaster-response\n",
      "1 Boeing loses more 737 MAX orders, eyes jet&#39;s U.S. return but Europe tariffs loom  $BA $UPS $AAL \n",
      "\n",
      "https://newsfilter.io/a/bb9fcd4b5c2150a923e6d6c8b52bc35a\n",
      "2 $UPS $FDX https://finance.yahoo.com/news/dhl-fedex-ups-ready-save-100806840.html\n",
      "\n",
      "GOING TO BE THE BEST QUARTER PT$200\n",
      "3 $UPS ups Going to be busiest ever this Christmas $200 üèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇÔ∏èüèÑüèø‚Äç‚ôÇ\n",
      "4 U.S. Justice Department clears Uber-Postmates deal\n",
      "\n",
      "https://pageone.ng/2020/11/10/u-s-justice-department-clears-uber-postmates-deal/\n",
      "\n",
      "$UBER $POSTMATES $LYFT $UPS $FDX\n"
     ]
    }
   ],
   "source": [
    "# sample request with AAPL\n",
    "r = requests.get('https://api.stocktwits.com/api/2/streams/symbol/'+\"UPS\"+\".json\");\n",
    "jsonText = json.dumps(r.json())\n",
    "                \n",
    "dictionary = json.loads(jsonText)\n",
    "maxi = dictionary['cursor']['max']          \n",
    "length = len(dictionary['messages'])\n",
    "\n",
    "print(\"Number of tweets on page: \"+str(length))\n",
    "\n",
    "for i in range(5):\n",
    "    message = dictionary['messages'][i]\n",
    "    x = message['body']\n",
    "    print(i,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is dirty so clean up the data: \n",
    "***No emoticons, No Links, No special characters etc.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    \n",
    "    # function deletes special characters \n",
    "    # deletes emoji's\n",
    "        \n",
    "    tweet = tweet.replace('\\n',' ').replace('\\r',' ') \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### ***Which Stocks to Scrape?***\n",
    "- Scrape the top 2000 USA market cap stocks\n",
    "- Found on Finviz: <a href=\"https://finviz.com/screener.ashx?v=111&f=geo_usa&o=-marketcap&r=21\">Screener</a>\n",
    "- <a href=\"http://atlaiser.com/phpmyadmin/index.php\">Most tweets are found in the top stocks</a>\n",
    "\n",
    "- ### ***How many tweets to scrape?***\n",
    "- Select 10% of the tweets from watchers\n",
    "\n",
    "<b><p>SELECT SUM(watchers) from tickers</p></b>\n",
    "\n",
    "- #### ***Goal of 1.55 Million Tweets was achieved***\n",
    "- Interesting insight: only about **20%** of tweets are **bearish**\n",
    "\n",
    "<b><p>SELECT COUNT(*) from tweets</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Optimizing the Scraper</h2>\n",
    "<h2 style=\"margin:0px;\">10,000/hr -> 20,000/hr -> 100,000/hr</h2>\n",
    "<h6 style=\"margin:0px;\">Sequential -> Multithreaded -> Multithreaded Database Insertion</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Code run-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
